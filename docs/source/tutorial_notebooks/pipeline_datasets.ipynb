{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9337f001-5e7c-4141-a60c-5e99052aee3d",
   "metadata": {},
   "source": [
    "<img src=\"../_static/DREGS_logo_v2.png\" width=\"300\"/>\n",
    "\n",
    "# Working with pipeline datasets\n",
    "\n",
    "This tutorial focuses on how to register data into the data registry from a\n",
    "complete end-to-end pipeline. A \"pipeline\" in this context is any collection of\n",
    "datasets that are inter-dependent, i.e., the output data from one process feeds\n",
    "into the next process as its starting point. For example, a pipeline could\n",
    "start with some raw imagery from a telescope, this raw imagery is then reduced\n",
    "and fed into a piece of software that outputs a human-friendly value added\n",
    "catalog. Or, a pipeline could be from a numerical simulation, starting with the\n",
    "simulation's initial conditions, which then feed into an N-body code, which\n",
    "then feed into a structure finder and gets reduced to a halo catalog.\n",
    "\n",
    "In the DESC data registry nomenclature, each stage of a pipeline is an\n",
    "\"**execution**\", the data product(s) produced during each execution are \"**datasets**\",\n",
    "and executions are linked to one another via \"**dependencies**\".\n",
    "\n",
    "### What we cover in this tutorial\n",
    "\n",
    "In this tutorial we will learn how to:\n",
    "\n",
    "- Register a series of dependant datasets from a pipeline into the data registry.\n",
    "\n",
    "### Before we begin\n",
    "\n",
    "If you haven't done so already, check out the getting setup page from the docs if you want to run this tutorial interactively.\n",
    "\n",
    "A quick way to check everything is set up correctly is to run the first cell below, which should load the `dataregistry` package, and print the package version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead9b84-4933-4213-93cb-301d79ef1167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dataregistry\n",
    "print(\"Working with dataregistry version:\", dataregistry.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48aec2e-2b35-49ed-be76-8818d9e79b2c",
   "metadata": {},
   "source": [
    "# A pipeline example\n",
    "\n",
    "For this example we have a pipeline comprising of three stages.\n",
    "\n",
    "In the first\n",
    "stage three datasets are produced (a directory structure and two individual files). The data output from the first stage\n",
    "feeds into the second stage as input, which in turn produces its own output (in\n",
    "this case a directory structure). Finally, the output data from stage two is\n",
    "fed into the third stage as input and produces its own output dataset directory\n",
    "structure. Thus our three stages have a simple sequential linking structure;\n",
    "`Stage1 -> Stage2` and `Stage2 -> Stage3`.\n",
    "\n",
    "Below is a graphical representation of the setup.\n",
    "\n",
    "<img src=\"../_static/pipeline_example.png\" width=\"600\"/>\n",
    "\n",
    "How then would we go about inputting the five datasets from this pipeline into the DESC data registry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3db889-e826-40f9-841d-f41a4cde3159",
   "metadata": {},
   "source": [
    "### Connect to the data registry\n",
    "\n",
    "To begin we establish a link to the data registry using the ``DataRegistry`` class (the Getting Started tutorial goes through this step in more detail).\n",
    "\n",
    "Note we are setting a global `owner` and `owner_type` here, which will be inherited automatically during each `register_dataset` call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12958ec5-87b0-47fc-9440-a34e493729c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataregistry import DataRegistry\n",
    "\n",
    "# Establish connection to database, setting a default owner and owner_type for all registered datasets in this instance.\n",
    "datareg = DataRegistry(owner=\"DESC CO Group\", owner_type=\"group\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e548190-fc4c-457e-9e5a-7a14b92a74cb",
   "metadata": {},
   "source": [
    "### Register the executions and datasets with DataRegistry\n",
    "\n",
    "Now we can enter our database entries, starting with an `execution` entry to\n",
    "represent the first stage of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85faaee8-bbc2-4879-93d5-ada69d2acbc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ex1_id = datareg.Registrar.register_execution(\n",
    "   \"pipeline-stage-1\",\n",
    "   description=\"The first stage of my pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7744a70-866d-419b-a083-4a4576b88727",
   "metadata": {},
   "source": [
    "where `ex1_id` is the `DataRegistry` index for this execution, which we will reference later.\n",
    "\n",
    "Next, we register the datasets associated with the output of `pipeline-stage-1`. Note we mark them as \"dummy\" datasets, this means that no data is copied (or even neeeds to exist), only a database entry is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d10980-cbd1-40cb-a9f5-45419b44df5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_id1 = datareg.Registrar.register_dataset(\n",
    "   \"pipeline_tutorial/dataset_1p1/\",\n",
    "   \"0.0.1\",\n",
    "   description=\"A directory structure output from pipeline stage 1\",\n",
    "   old_location=\"/somewhere/on/machine/my-dataset/\",\n",
    "   execution_id=ex1_id,\n",
    "   name=\"Dataset 1.1\",\n",
    "   is_overwritable=True,\n",
    "   is_dummy=True\n",
    ")\n",
    "\n",
    "dataset_id2 = datareg.Registrar.register_dataset(\n",
    "   \"pipeline_tutorial/dataset_1p2.db\",\n",
    "   \"0.0.1\",\n",
    "   description=\"A file output from pipeline stage 1\",\n",
    "   old_location=\"/somewhere/on/machine/other-datasets/database.db\",\n",
    "   execution_id=ex1_id,\n",
    "   name=\"Dataset 1.2\",\n",
    "   is_overwritable=True,\n",
    "   is_dummy=True\n",
    ")\n",
    "\n",
    "dataset_id3 = datareg.Registrar.register_dataset(\n",
    "   \"pipeline_tutorial/dataset_1p3.hdf5\",\n",
    "   \"0.0.1\",\n",
    "   description=\"Another file output from pipeline stage 1\",\n",
    "   old_location=\"/somewhere/on/machine/other-datasets/info.hdf5\",\n",
    "   execution_id=ex1_id,\n",
    "   name=\"Dataset 1.3\",\n",
    "   is_overwritable=True,\n",
    "   is_dummy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65481996-cf41-4703-9a18-46d3c8193f3e",
   "metadata": {},
   "source": [
    "Now, the `execution` for stage two of our pipeline. Note this will\n",
    "automatically generate a dependency between the two executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa5a7f-ccb1-47d3-9e9b-b62ad32287d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ex2_id = datareg.Registrar.register_execution(\n",
    "   \"pipeline-stage-2\",\n",
    "   description=\"The second stage of my pipeline\",\n",
    "   input_datasets=[dataset_id1,dataset_id2,dataset_id3],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed3ffb-02f2-42ec-b2c0-643c51f8a295",
   "metadata": {
    "tags": []
   },
   "source": [
    "and then to finish, we repeat the process for the remaining datasets and\n",
    "remaining execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a01c97-a9f5-49b5-b8de-83712ac5f7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_id4 = datareg.Registrar.register_dataset(\n",
    "    \"pipeline_tutorial/dataset_2p1\",\n",
    "    \"0.0.1\",\n",
    "    description=\"A directory structure output from pipeline stage 2\",\n",
    "    old_location=\"/somewhere/on/machine/my-second-dataset/\",\n",
    "    execution_id=ex2_id,\n",
    "    name=\"Dataset 2.1\",\n",
    "    is_overwritable=True,\n",
    "    is_dummy=True\n",
    ")\n",
    "\n",
    "ex3_id = datareg.Registrar.register_execution(\n",
    "    \"pipeline-stage-3\",\n",
    "    description=\"The third stage of my pipeline\",\n",
    "    input_datasets=[dataset_id4],\n",
    ")\n",
    "\n",
    "dataset_id5 = datareg.Registrar.register_dataset(\n",
    "    \"pipeline_tutorial/dataset_3p1\",\n",
    "    \"0.0.1\",\n",
    "    description=\"A directory structure output from pipeline stage 3\",\n",
    "    old_location=\"/somewhere/on/machine/my-third-dataset/\",\n",
    "    execution_id=ex3_id,\n",
    "    name=\"Dataset 3.1\",\n",
    "    is_overwritable=True,\n",
    "    is_dummy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a0ccc-3985-44dc-934d-9854b806f4dd",
   "metadata": {},
   "source": [
    "# Querying a pipeline dataset\n",
    "\n",
    "Coming soon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c298bf-7d44-4c33-b7d2-2fe75d0c8d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
