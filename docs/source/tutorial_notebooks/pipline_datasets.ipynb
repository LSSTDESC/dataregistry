{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9337f001-5e7c-4141-a60c-5e99052aee3d",
   "metadata": {},
   "source": [
    "<img src=\"../_static/DREGS_logo.png\" width=\"300\"/>\n",
    "\n",
    "# Working with pipeline datasets\n",
    "\n",
    "This tutorial focuses on how to register data into `DREGS` from a\n",
    "complete end-to-end pipeline. A \"pipeline\" in this context is any collection of\n",
    "datasets that are inter-dependent, i.e., the output data from one process feeds\n",
    "into the next process as its starting point. For example, a pipeline could\n",
    "start with some raw imagery from a telescope, this raw imagery is then reduced\n",
    "and fed into a piece of software that outputs a human-friendly value added\n",
    "catalog. Or, a pipeline could be from a numerical simulation, starting with the\n",
    "simulation's initial conditions, which then feed into an N-body code, which\n",
    "then feed into a structure finder and gets reduced to a halo catalog.\n",
    "\n",
    "In the DESC data registry nomenclature, each stage of a pipeline is an\n",
    "\"**execution**\", the data product(s) produced during each execution are \"**datasets**\",\n",
    "and executions are linked to one another via \"**dependencies**\".\n",
    "\n",
    "### What we cover in this tutorial\n",
    "\n",
    "In this tutorial we will learn how to:\n",
    "\n",
    "- Register a series of dependant datasets from a pipeline into DREGS\n",
    "\n",
    "### Before we begin\n",
    "\n",
    "If you haven't done so already, check out the [getting setup](file:///home/mcalpine/Documents/dataregistry/dataregistry/docs/build/html/tutorial_setup.html) page from the docs if you want to run this tutorial interactively.\n",
    "\n",
    "A quick way to check everything is set up correctly is to run the first cell below, which should load the `dataregistry` package, and print the package version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ead9b84-4933-4213-93cb-301d79ef1167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with dataregistry version: 0.2.2\n"
     ]
    }
   ],
   "source": [
    "import dataregistry\n",
    "print(\"Working with dataregistry version:\", dataregistry.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48aec2e-2b35-49ed-be76-8818d9e79b2c",
   "metadata": {},
   "source": [
    "## A pipeline example\n",
    "\n",
    "For this example we have a pipeline comprising of three stages.\n",
    "\n",
    "In the first\n",
    "stage three datasets are produced (a directory structure and two individual files). The data output from the first stage\n",
    "feeds into the second stage as input, which in turn produces its own output (in\n",
    "this case a directory structure). Finally, the output data from stage two is\n",
    "fed into the third stage as input and produces its own output dataset directory\n",
    "structure. Thus our three stages have a simple sequential linking structure;\n",
    "`Stage1 -> Stage2` and `Stage2 -> Stage3`.\n",
    "\n",
    "Below is a graphical representation of the setup.\n",
    "\n",
    "<img src=\"../_static/pipeline_example.png\" width=\"600\"/>\n",
    "\n",
    "How then would we go about inputting the five datasets from this pipeline into the DESC data registry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2ac22",
   "metadata": {},
   "source": [
    "### Connect to the database\n",
    "\n",
    "To begin we need to get set up; importing the `DREGS` class (see the \"Getting started with DREGS tutorial for a bit more detail about this stage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66a6f3ac-15cc-4706-b230-63681ba3a4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataregistry import DREGS\n",
    "\n",
    "# Establish connection to database (using defaults) \n",
    "dregs = DREGS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e548190-fc4c-457e-9e5a-7a14b92a74cb",
   "metadata": {},
   "source": [
    "### Register the executions and datasets with DREGS\n",
    "\n",
    "Now we can enter our database entries, starting with an `execution` entry to\n",
    "represent the first stage of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85faaee8-bbc2-4879-93d5-ada69d2acbc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ex1_id = dregs.Registrar.register_execution(\n",
    "   \"pipeline-stage-1\",\n",
    "   description=\"The first stage of my pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7744a70-866d-419b-a083-4a4576b88727",
   "metadata": {},
   "source": [
    "where ``ex1_id`` is the `DREGS` index for this execution, which we will reference later.\n",
    "\n",
    "Next, we register the datasets associated with the output of\n",
    "``pipeline-stage-1``. Each dataset by default (as we have not specified\n",
    "otherwise) will be entered with ``owner=$USER`` and ``owner_type=user``. Note we mark them as \"dummy\" datasets, this means that no data is copied, only a database entry is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09d10980-cbd1-40cb-a9f5-45419b44df5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_id1 = dregs.Registrar.register_dataset(\n",
    "   \"dregs_pipeline_tutorial/dataset_1p1/\",\n",
    "   \"0.0.1\",\n",
    "   description=\"A directory structure output from pipeline stage 1\",\n",
    "   old_location=\"/somewhere/on/machine/my-dataset/\",\n",
    "   execution_id=ex1_id,\n",
    "   name=\"Dataset 1.1\",\n",
    "   is_overwritable=True,\n",
    "   is_dummy=True\n",
    ")\n",
    "\n",
    "dataset_id2 = dregs.Registrar.register_dataset(\n",
    "   \"dregs_pipeline_tutorial/dataset_1p2.db\",\n",
    "   \"0.0.1\",\n",
    "   description=\"A file output from pipeline stage 1\",\n",
    "   old_location=\"/somewhere/on/machine/other-datasets/database.db\",\n",
    "   execution_id=ex1_id,\n",
    "   name=\"Dataset 1.2\",\n",
    "   is_overwritable=True,\n",
    "   is_dummy=True\n",
    ")\n",
    "\n",
    "dataset_id3 = dregs.Registrar.register_dataset(\n",
    "   \"dregs_pipeline_tutorial/dataset_1p3.hdf5\",\n",
    "   \"0.0.1\",\n",
    "   description=\"Another file output from pipeline stage 1\",\n",
    "   old_location=\"/somewhere/on/machine/other-datasets/info.hdf5\",\n",
    "   execution_id=ex1_id,\n",
    "   name=\"Dataset 1.3\",\n",
    "   is_overwritable=True,\n",
    "   is_dummy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65481996-cf41-4703-9a18-46d3c8193f3e",
   "metadata": {},
   "source": [
    "Now, the `execution` for stage two of our pipeline. Note this will\n",
    "automatically generate a dependency between the two executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aaa5a7f-ccb1-47d3-9e9b-b62ad32287d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ex2_id = dregs.Registrar.register_execution(\n",
    "   \"pipeline-stage-2\",\n",
    "   description=\"The second stage of my pipeline\",\n",
    "   input_datasets=[dataset_id1,dataset_id2,dataset_id3],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed3ffb-02f2-42ec-b2c0-643c51f8a295",
   "metadata": {
    "tags": []
   },
   "source": [
    "and then to finish, we repeat the process for the remaining datasets and\n",
    "remaining execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a01c97-a9f5-49b5-b8de-83712ac5f7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_id4 = dregs.Registrar.register_dataset(\n",
    "    \"dregs_pipeline_tutorial/dataset_2p1\",\n",
    "    \"0.0.1\",\n",
    "    description=\"A directory structure output from pipeline stage 2\",\n",
    "    old_location=\"/somewhere/on/machine/my-second-dataset/\",\n",
    "    execution_id=ex2_id,\n",
    "    name=\"Dataset 2.1\",\n",
    "    is_overwritable=True,\n",
    "    is_dummy=True\n",
    ")\n",
    "\n",
    "ex3_id = dregs.Registrar.register_execution(\n",
    "    \"pipeline-stage-3\",\n",
    "    description=\"The third stage of my pipeline\",\n",
    "    input_datasets=[dataset_id4],\n",
    ")\n",
    "\n",
    "dataset_id5 = dregs.Registrar.register_dataset(\n",
    "    \"dregs_pipeline_tutorial/dataset_3p1\",\n",
    "    \"0.0.1\",\n",
    "    description=\"A directory structure output from pipeline stage 3\",\n",
    "    old_location=\"/somewhere/on/machine/my-third-dataset/\",\n",
    "    execution_id=ex3_id,\n",
    "    name=\"Dataset 3.1\",\n",
    "    is_overwritable=True,\n",
    "    is_dummy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a0ccc-3985-44dc-934d-9854b806f4dd",
   "metadata": {},
   "source": [
    "## Querying a pipeline dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9901d89-b1d7-48c9-8110-ce16ecba3a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a filter that queries on the dataset name\n",
    "f = dregs.Query.gen_filter('dataset.name', '==', 'my_desc_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a8df6-6967-4280-a5e8-6ea8831eff09",
   "metadata": {},
   "source": [
    "Like with SQL, column names can either be explicit, or not, with the prefix of their table name. For example `name` rather than `dataset.name`. However this would only be valid if the column `name` was unique across all tables, which it is not. We would always recommend being explicit, and including the table name with filters.\n",
    "\n",
    "Now we can pass this filter through to a query using the `Query` extension of the `DREGS` class, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00c6d355-dca0-42a1-ae82-7fdbd1a46afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query the database\n",
    "results = dregs.Query.find_datasets(['dataset.dataset_id', 'dataset.name', 'dataset.relative_path'], [f])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc05dc6-43e9-4d10-af44-0e4a9353c0b4",
   "metadata": {},
   "source": [
    "Which takes a list of column names we want to return, and a list of filter objects for the query.\n",
    "\n",
    "A SQLAlchemy object is returned, we can look at the results like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0841f472-4ae6-4ca1-810d-6996c58fa14a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 'my_desc_dataset', 'my_desc_project/my_desc_dataset')\n",
      "(6, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(7, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(8, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(9, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(18, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(10, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(12, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(14, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(16, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(17, 'my_desc_dataset', 'dregs_nersc_tutorial/my_updated_desc_dataset')\n",
      "(19, 'my_desc_dataset', 'dregs_nersc_tutorial/my_updated_desc_dataset')\n",
      "(11, 'my_desc_dataset', 'dregs_nersc_tutorial/my_updated_desc_dataset')\n",
      "(13, 'my_desc_dataset', 'dregs_nersc_tutorial/my_updated_desc_dataset')\n",
      "(15, 'my_desc_dataset', 'dregs_nersc_tutorial/my_updated_desc_dataset')\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f5445",
   "metadata": {},
   "source": [
    "To get a list of all columns in the database, along with what table they belong to, you can use the `Query.get_all_columns()` function, i.e.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a52029-2908-4056-bc68-4a87f6c3e6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset.dataset_id', 'dataset.name', 'dataset.relative_path', 'dataset.version_major', 'dataset.version_minor', 'dataset.version_patch', 'dataset.version_string', 'dataset.version_suffix', 'dataset.dataset_creation_date', 'dataset.is_archived', 'dataset.is_external_link', 'dataset.is_overwritable', 'dataset.is_overwritten', 'dataset.is_valid', 'dataset.register_date', 'dataset.creator_uid', 'dataset.access_API', 'dataset.execution_id', 'dataset.description', 'dataset.owner_type', 'dataset.owner', 'dataset.data_org', 'dataset.nfiles', 'dataset.total_disk_space', 'execution.execution_id', 'execution.description', 'execution.register_date', 'execution.execution_start', 'execution.name', 'execution.locale', 'execution.configuration', 'execution.creator_uid', 'dataset_alias.dataset_alias_id', 'dataset_alias.alias', 'dataset_alias.dataset_id', 'dataset_alias.supersede_date', 'dataset_alias.register_date', 'dataset_alias.creator_uid', 'dependency.dependency_id', 'dependency.register_date', 'dependency.input_id', 'dependency.execution_id']\n"
     ]
    }
   ],
   "source": [
    "print(dregs.Query.get_all_columns())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DREGS-env",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
