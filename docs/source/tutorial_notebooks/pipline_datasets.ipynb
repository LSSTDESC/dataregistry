{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9337f001-5e7c-4141-a60c-5e99052aee3d",
   "metadata": {},
   "source": [
    "<img src=\"../_static/DREGS_logo.png\" width=\"300\"/>\n",
    "\n",
    "# Working with pipeline datasets\n",
    "\n",
    "This tutorial focuses on how to register data into `DREGS` from a\n",
    "complete end-to-end pipeline. A \"pipeline\" in this context is any collection of\n",
    "datasets that are inter-dependent, i.e., the output data from one process feeds\n",
    "into the next process as its starting point. For example, a pipeline could\n",
    "start with some raw imagery from a telescope, this raw imagery is then reduced\n",
    "and fed into a piece of software that outputs a human-friendly value added\n",
    "catalog. Or, a pipeline could be from a numerical simulation, starting with the\n",
    "simulation's initial conditions, which then feed into an N-body code, which\n",
    "then feed into a structure finder and gets reduced to a halo catalog.\n",
    "\n",
    "In the DESC data registry nomenclature, each stage of a pipeline is an\n",
    "\"execution\", the data product(s) produced during each execution are \"datasets\",\n",
    "and executions are linked to one another via \"dependencies\".\n",
    "\n",
    "### What we cover in this tutorial\n",
    "\n",
    "In this tutorial we will learn how to:\n",
    "\n",
    "- Register a series of dependant datasets from a pipeline into DREGS\n",
    "\n",
    "### Before we begin\n",
    "\n",
    "If you haven't done so already, check out the [getting setup](file:///home/mcalpine/Documents/dataregistry/dataregistry/docs/build/html/tutorial_setup.html) page from the docs if you want to run this tutorial interactively.\n",
    "\n",
    "A quick way to check everything is set up correctly is to run the first cell below, which should load the `dataregistry` package, and print the package version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ead9b84-4933-4213-93cb-301d79ef1167",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with dataregistry version: 0.2.2\n"
     ]
    }
   ],
   "source": [
    "import dataregistry\n",
    "print(\"Working with dataregistry version:\", dataregistry.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48aec2e-2b35-49ed-be76-8818d9e79b2c",
   "metadata": {},
   "source": [
    "## A pipeline example\n",
    "\n",
    "For this example, we have a pipeline comprising of three stages. In the first\n",
    "stage three datasets are produced, one dataset is a directory structure, and\n",
    "the remaining two are individual files. The data output from the first stage\n",
    "feeds into the second stage as input, which in turn produces its own output (in\n",
    "this case a directory structure). Finally, the output data from stage two is\n",
    "fed into the third stage as input and produces its own output dataset directory\n",
    "structure. Thus our three stages have a simple sequential linking structure;\n",
    "`Stage1 -> Stage2` and `Stage2 -> Stage3`.\n",
    "\n",
    "Below is a graphical representation of the setup.\n",
    "\n",
    "<img src=\"../_static/pipeline_example.png\" width=\"300\"/>\n",
    "\n",
    "How then would we go about inputting this pipeline into the DESC data registry?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2ac22",
   "metadata": {},
   "source": [
    "To begin we need to get set up; importing the `DREGS` class. As with the\n",
    "standalone dataset example, we are assuming the default DREGS configuration\n",
    "(see the :ref:`Installation page <installation>` for more details).\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "   from dataregistry import DREGS\n",
    "\n",
    "   # Establish connection to database (using defaults) \n",
    "   dregs = DREGS()\n",
    "\n",
    "Now we can enter our database entries, starting with an `execution` entry to\n",
    "represent the first stage of our pipeline.\n",
    "\n",
    ".. code-block:: python\n",
    "\n",
    "   ex1_id = dregs.Registrar.register_execution(\n",
    "       \"pipeline-stage-1\",\n",
    "       description=\"The first stage of my pipeline\",\n",
    "   )\n",
    "\n",
    "where ``ex1_id`` is the `DREGS` index for this execution which we will reference later.\n",
    "\n",
    "Next, we register the datasets associated with the output of\n",
    "``pipeline-stage-1``. Each dataset by default (as we have not specified\n",
    "otherwise) will be entered with ``owner=$USER`` and ``owner_type=user``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66a6f3ac-15cc-4706-b230-63681ba3a4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataregistry import DREGS\n",
    "\n",
    "# Establish connection to database (using defaults) \n",
    "dregs = DREGS()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7423fb-32d0-4a33-8e87-cd75e952512f",
   "metadata": {},
   "source": [
    "However, if you are not connecting to the default DREGS database, or if your configuration file is located somewhere other than your `$HOME` directory, you must provide that information during the object creation, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85faaee8-bbc2-4879-93d5-ada69d2acbc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# When your configuration file is not in the default location\n",
    "# dregs = DREGS(config_file=\"/path/to/config\")\n",
    "\n",
    "# If you are connecting to a database other than the DREGS default, you need to specify the schema name to connect to\n",
    "# dregs = DREGS(schema_version=\"myschema\")\n",
    "\n",
    "# If you want to specify the root_dir that data is copied to (this should only be changed for testing purposes)\n",
    "# dregs = DREGS(root_dir=\"/my/root/dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7744a70-866d-419b-a083-4a4576b88727",
   "metadata": {},
   "source": [
    "When creating a `DREGS` class instance which you intend to use for registering datasets, you may optionally set a default `owner` and/or `owner_type` that will be inherited for all datasets that are registered during that instance. See details in the section \"*Registering new datasets with DREGS*\" below for details about `owner` and `owner_type`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d10980-cbd1-40cb-a9f5-45419b44df5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting a global owner and owner_type default value for all datasets that will be registered during this instance\n",
    "# dregs = DREGS(owner=\"desc\", owner_type=\"production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d723a37-4101-496c-b385-0a2644aa7ad8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Registering new datasets with DREGS\n",
    "\n",
    "Now that we have made our connection to the database we can register some datasets using the `Registrar` extension of the `DREGS` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aaa5a7f-ccb1-47d3-9e9b-b62ad32287d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a empty text file\n",
    "with open(\"dummy_dataset.txt\", \"w\") as f:\n",
    "    f.write(\"some data\")\n",
    "\n",
    "# Add new entry.\n",
    "dataset_id = dregs.Registrar.register_dataset(\n",
    "    \"dregs_nersc_tutorial/my_desc_dataset\",\n",
    "    \"1.0.0\",\n",
    "    description=\"An output from some DESC code\",\n",
    "    owner=\"DESC\",\n",
    "    owner_type=\"group\",\n",
    "    is_overwritable=True,\n",
    "    old_location=\"dummy_dataset.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed3ffb-02f2-42ec-b2c0-643c51f8a295",
   "metadata": {
    "tags": []
   },
   "source": [
    "This will register a new dataset into DREGS. A few notes:\n",
    "\n",
    "### The relative path\n",
    "\n",
    "Datasets are registered in the data registry under a relative path. For those interested, the eventual full path for the dataset will be `<root_dir>/<owner_type>/<owner>/<relative_path>`. The relative path is one of the two required parameters you must specify when registering a dataset (in the example here our relative path is `dregs_nersc_tutorial/my_desc_dataset`).\n",
    "\n",
    "By default datasets are non-overwritable, therefore relative paths for a given `owner` and `owner_type` must be unique in that case (we have allowed this example dataset to be overwritable so that it does not raise an error through multiple tests).\n",
    "\n",
    "### The version string\n",
    "\n",
    "The second required parameter is the version string, in the semantic format, i.e., MAJOR.MINOR.PATCH. There is also the optional `version_suffix` parameter which can be used to add a suffix to the version string.\n",
    "\n",
    "### Owner and Owner type\n",
    "\n",
    "Datasets are registered under a given `owner`. This can be any string, however it should be informative. If no `owner` is specified, and no global `owner` was set when the `DREGS` instance was created, `$USER` is used as the default.\n",
    "\n",
    "One further level of classification is the `owner_type`, which can be one of `user`, `group`, `project` or `production`. If `owner_type` is not specified, and no global `owner_type` was set when the `DREGS` instance was created, `user` is the default. Note that production datasets can never be overwritten (even if `is_overwritable=True`).\n",
    "\n",
    "### Copying the data\n",
    "\n",
    "Registering a dataset with DREGS does two things; it creates an entry in the DESC data registry database with the appropriate metadata, and it (optionally) copies the dataset contents to a central shared space at NERSC (i.e., under `<root_dir>`). \n",
    "\n",
    "If the data are already at the correct relative path under the NERSC shared space, then only the relative path needs to be provided, and the dataset will then be registered. However it's likely for most users the data will need to be copied from another location at NERSC to the DREGS shared space. That initial location may be specified using the `old_location` parameter. \n",
    "\n",
    "In our example we have created a dummy text file as our dataset and ingested it into the data registry, however this can be any file or directory (directories will be recursively copied) at NERSC.\n",
    "\n",
    "### Extra options\n",
    "\n",
    "All the `register_dataset` parameters we do not explicitly specify revert to their default values. For a full list of these options see the documentation [here](http://lsstdesc.org/dataregistry/reference_python.html#the-dregs-class). We would advise you to be as precise as possible when creating entries within DREGS. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6b7862-fd31-43e6-9a50-572b5cd6fff5",
   "metadata": {},
   "source": [
    "## Updating a previously registered dataset with a newer version\n",
    "\n",
    "If a dataset previously registered with DREGS gets updated, you have three options for how to handle the new entry into DREGS:\n",
    "\n",
    "- You can enter it as a completely new standalone dataset with no links to the previous dataset\n",
    "- You can overwrite the existing dataset with the new data (only if the previous dataset was entered with `is_overwritable=True`)\n",
    "- You can enter it as a new version of the previous dataset (recommended)\n",
    "\n",
    "Unless you are overwriting a previous dataset, you cannot enter a new dataset (even an updated version) using the same relative path. However, datasets can share the same `name` field, which is what we'll use to keep our updated dataset connected to our previous one.\n",
    "\n",
    "Note that in the original dataset we did not specify `name` during registration. The default name for a dataset is the file or directory name taken from its relative path. In our example above this would be `my_desc_dataset`.\n",
    "\n",
    "The combination of `name`, `version` and `version_suffix` for any dataset in DREGS must be unique. As we are updating a dataset with the same name, we have to make sure to update the version to a new value. One handy feature is automatic version \"bumping\" for datasets, i.e., rather than specifying a new version string manually, one can select \"major\", \"minor\" or \"patch\" for the version string to automatically bump it up. In our case, selecting \"minor\" will automatically generate the version \"1.1.0\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a01c97-a9f5-49b5-b8de-83712ac5f7f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add new entry for an updated dataset with an updated version.\n",
    "dataset_id = dregs.Registrar.register_dataset(\n",
    "    \"dregs_nersc_tutorial/my_updated_desc_dataset\",\n",
    "    \"minor\", # Automatically bumps to \"1.1.0\"\n",
    "    description=\"An output from some DESC code (updated)\",\n",
    "    is_overwritable=True,\n",
    "    old_location=\"dummy_dataset.txt\",\n",
    "    name=\"my_desc_dataset\" # Using this name links it to the previous dataset.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf6d38e-ea5e-4916-aa5d-654b2587942d",
   "metadata": {},
   "source": [
    "## Querying the data registry\n",
    "\n",
    "Now that we have covered the basics of dataset registration, we can have a look at how to query entries in the DREGS database.\n",
    "\n",
    "Queries are constructed from one or more boolean logic \"filters\", which translate to SQL `WHERE` clauses in the code. \n",
    "\n",
    "For example, to create a filter that will query for all datasets in DREGS with the name \"my_desc_dataset\" would be as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9901d89-b1d7-48c9-8110-ce16ecba3a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a filter that queries on the dataset name\n",
    "f = dregs.Query.gen_filter('dataset.name', '==', 'my_desc_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305a8df6-6967-4280-a5e8-6ea8831eff09",
   "metadata": {},
   "source": [
    "Like with SQL, column names can either be explicit, or not, with the prefix of their table name. For example `name` rather than `dataset.name`. However this would only be valid if the column `name` was unique across all tables, which it is not. We would always recommend being explicit, and including the table name with filters.\n",
    "\n",
    "Now we can pass this filter through to a query using the `Query` extension of the `DREGS` class, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c6d355-dca0-42a1-ae82-7fdbd1a46afa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Query the database\n",
    "results = dregs.Query.find_datasets(['dataset.dataset_id', 'dataset.name', 'dataset.relative_path'], [f])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc05dc6-43e9-4d10-af44-0e4a9353c0b4",
   "metadata": {},
   "source": [
    "Which takes a list of column names we want to return, and a list of filter objects for the query.\n",
    "\n",
    "A SQLAlchemy object is returned, we can look at the results like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0841f472-4ae6-4ca1-810d-6996c58fa14a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 'my_desc_dataset', 'my_desc_project/my_desc_dataset')\n",
      "(6, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(7, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(8, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(9, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(10, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(12, 'my_desc_dataset', 'dregs_nersc_tutorial/my_desc_dataset')\n",
      "(11, 'my_desc_dataset', 'dregs_nersc_tutorial/my_updated_desc_dataset')\n",
      "(13, 'my_desc_dataset', 'dregs_nersc_tutorial/my_updated_desc_dataset')\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f5445",
   "metadata": {},
   "source": [
    "To get a list of all columns in the database, along with what table they belong to, you can use the `Query.get_all_columns()` function, i.e.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a52029-2908-4056-bc68-4a87f6c3e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dregs.Query.get_all_columns())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
