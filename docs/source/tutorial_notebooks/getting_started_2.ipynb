{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9337f001-5e7c-4141-a60c-5e99052aee3d",
   "metadata": {},
   "source": [
    "<div style=\"overflow: hidden;\">\n",
    "    <img src=\"images/DREGS_logo_v2.png\" width=\"300\" style=\"float: left; margin-right: 10px;\">\n",
    "</div>\n",
    "\n",
    "# Getting started with the DESC data registry (part 2)\n",
    "\n",
    "Here we continue our getting started tutorial, introducing \"executions\" and \"dependencies\".\n",
    "\n",
    "### What we cover in this tutorial\n",
    "\n",
    "In this tutorial we will learn how to:\n",
    "\n",
    "- Create a new execution and assign datasets to it\n",
    "- Connect executions through dependencies\n",
    "\n",
    "### Before we begin\n",
    "\n",
    "If you haven't done so already, check out the [getting setup](https://lsstdesc.org/dataregistry/tutorial_setup.html) page from the documentation if you want to run this tutorial interactively.\n",
    "\n",
    "A quick way to check everything is set up correctly is to run the first cell below, which should load the `dataregistry` package, and print the package version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ead9b84-4933-4213-93cb-301d79ef1167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dataregistry\n",
    "print(\"Working with dataregistry version:\", dataregistry.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48aec2e-2b35-49ed-be76-8818d9e79b2c",
   "metadata": {},
   "source": [
    "## Executions\n",
    "\n",
    "The `dataregistry` nomenclature for a grouping of datasets is an \"execution\". For example, when multiple datasets are produced from a DESC pipeline stage, or numerical simulation, say, the parent pipeline stage or numerical simulation would be the \"execution\", and we associate the individual datasets as child members of that execution.\n",
    "\n",
    "Executions have their own entries and associated metadata in the dataregistry. Those execution entries must be registered first, then during the dataset registration we can associate the datasets with their parent execution entry.\n",
    "\n",
    "By default, if a dataset is not assigned a user-created execution during registration, a stand alone execution is generated for the dataset automatically. Therefore if your execution produces a single output, i.e., the dataset you are registering, you do not need to worry about also creating a separate execution entry for it. Executions are always necessary to link dependencies, which we will cover later in the tutorial, which is why all datasets must have one.  \n",
    "\n",
    "To create an execution we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6f3ac-15cc-4706-b230-63681ba3a4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataregistry import DataRegistry\n",
    "\n",
    "# Establish connection to database (using defaults)\n",
    "datareg = DataRegistry()\n",
    "\n",
    "# Register a new execution\n",
    "ex1_id = datareg.Registrar.register_execution(\n",
    "   \"pipeline-stage-1\",\n",
    "   description=\"The first stage of my pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7423fb-32d0-4a33-8e87-cd75e952512f",
   "metadata": {},
   "source": [
    "where `ex1_id` is the `DataRegistry` index for this execution, which we will need to associate the datasets with this execution.\n",
    "\n",
    "All executions require a `name`, in our case \"pipeline-stage-1\". We have also provided an optional description. For a full list of metadata options for executions see the reference documentation [here](https://lsstdesc.org/dataregistry/reference_python.html).\n",
    "\n",
    "Now when we register a new dataset relating to this execution we just need to provide the execution ID, e.g.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55abc1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id1 = datareg.Registrar.register_dataset(\n",
    "   \"pipeline_tutorial/dataset_1p1/\",\n",
    "   \"0.0.1\",\n",
    "   description=\"A directory structure output from pipeline stage 1\",\n",
    "   old_location=\"/somewhere/on/machine/my-dataset/\",\n",
    "   execution_id=ex1_id,\n",
    "   name=\"Dataset 1.1\",\n",
    "   is_overwritable=True,\n",
    "   is_dummy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec12ad5",
   "metadata": {},
   "source": [
    "This is largely the same as the previous tutorial for registering a dataset, however now we are manually specifying the parent execution (`execution_id=ex1_id`).\n",
    "\n",
    "Note `is_dummy=True` is a flag to ignore the data at `old_location` (i.e., nothing is copied), and just create an entry in the database. This is a flag for testing purposes only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d723a37-4101-496c-b385-0a2644aa7ad8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dependencies\n",
    "\n",
    "Executions represent groupings of datasets connected to a single \"run\" (such as the output from a single stage of a DESC pipeline). Datasets can also be linked to one another via \"dependencies\", to classify them as a precursor dataset within the scope of a larger pipeline. Dependencies are created between datasets through their execution, however note that not all the datasets within the precursor execution are required to be dependencies of the datasets in the following execution.\n",
    "\n",
    "As with executions, dependencies are their own entry in the data registry, however they are generated automatically with the registration of executions (via the `input_datasets` option), so the user never needs to deal with creating dependencies directly.\n",
    "\n",
    "Take for example this simple pipeline, with three stages. Dataset 1.1 created from the first execution is a precursor dataset to the second execution, and Dataset 2.1 is a precursor dataset to the third execution:\n",
    "\n",
    "<div style=\"overflow: hidden;\">\n",
    "    <img src=\"images/pipeline_example.png\" width=\"800\" style=\"float: left; margin-right: 10px;\">\n",
    "</div>\n",
    "\n",
    "The DESC CO Group wants to enter this into the data registry, they would do it like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaa5a7f-ccb1-47d3-9e9b-b62ad32287d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataregistry import DataRegistry\n",
    "\n",
    "# Establish connection to database, setting a default owner and owner_type for all registered datasets in this instance.\n",
    "datareg = DataRegistry(owner=\"DESC CO Group\", owner_type=\"group\")\n",
    "\n",
    "# Create execution for first pipeline stage\n",
    "ex1_id = datareg.Registrar.register_execution(\n",
    "   \"pipeline-stage-1\"\n",
    ")\n",
    "\n",
    "# Register datasets with first pipeline stage.\n",
    "dataset_id1 = datareg.Registrar.register_dataset(\n",
    "   \"pipeline_tutorial/dataset_1p1/\",\n",
    "   \"0.0.1\",\n",
    "   execution_id=ex1_id,\n",
    "   is_overwritable=True,\n",
    "   is_dummy=True\n",
    ")\n",
    "\n",
    "dataset_id2 = datareg.Registrar.register_dataset(\n",
    "   \"pipeline_tutorial/dataset_1p2.db\",\n",
    "   \"0.0.1\",\n",
    "   execution_id=ex1_id,\n",
    "   is_overwritable=True,\n",
    "   is_dummy=True\n",
    ")\n",
    "\n",
    "dataset_id3 = datareg.Registrar.register_dataset(\n",
    "   \"pipeline_tutorial/dataset_1p3.hdf5\",\n",
    "   \"0.0.1\",\n",
    "   execution_id=ex1_id,\n",
    "   is_overwritable=True,\n",
    "   is_dummy=True\n",
    ")\n",
    "\n",
    "# Create execution for second pipeline stage\n",
    "ex2_id = datareg.Registrar.register_execution(\n",
    "   \"pipeline-stage-2\",\n",
    "   input_datasets=[dataset_id1,dataset_id2,dataset_id3]\n",
    ")\n",
    "\n",
    "# Register datasets with second pipeline stage\n",
    "dataset_id4 = datareg.Registrar.register_dataset(\n",
    "    \"pipeline_tutorial/dataset_2p1\",\n",
    "    \"0.0.1\",\n",
    "    execution_id=ex2_id,\n",
    "    is_overwritable=True,\n",
    "    is_dummy=True\n",
    ")\n",
    "\n",
    "# Create execution for third pipeline stage\n",
    "ex3_id = datareg.Registrar.register_execution(\n",
    "    \"pipeline-stage-3\",\n",
    "    input_datasets=[dataset_id4],\n",
    ")\n",
    "\n",
    "# Register datasets with third pipeline stage\n",
    "dataset_id5 = datareg.Registrar.register_dataset(\n",
    "    \"pipeline_tutorial/dataset_3p1\",\n",
    "    \"0.0.1\",\n",
    "    execution_id=ex3_id,\n",
    "    is_overwritable=True,\n",
    "    is_dummy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7bdbc5",
   "metadata": {},
   "source": [
    "We have skipped all the optional entries, such as dataset ``description``'s or ``execution_start``'s, for clarity, however we recommend being as thorough as possible when registering your entries into the registry.\n",
    "\n",
    "Note we never explicitly created any dependencies, they are automatically created because of the lines `input_datasets=[dataset_id1]` and `input_datasets=[dataset_id4]`.\n",
    "\n",
    "During pipeline queries, these dependencies will be internally used to return all the associated datasets with a given pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ccef1",
   "metadata": {},
   "source": [
    "## Querying executions and pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ef3d90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
